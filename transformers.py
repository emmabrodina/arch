# -*- coding: utf-8 -*-
"""transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12LQYuKirm9dw4hDalwP05ZVo-Y0BcLQR

##1. Завантаження даних
"""

# Завантаження файлу (використовуємо твір "Лис Микита" Івана Франка)
from google.colab import files

uploaded = files.upload()

# Зчитування тексту
with open("Lys_mykyta.txt", 'r', encoding='utf-8') as f:
    text = f.read()

# Виведемо приклад
print("Перші 1000 символів тексту:")
print(text[:1000])

# Обрахунок загальної довжини
total_chars = len(text)
unique_chars = len(set(text))

print(f"Всього символів у тексті: {total_chars}")
print(f"Унікальних символів: {unique_chars}")

"""##2. Очищення та токенізація"""

import re
import torch
from transformers import XLMRobertaTokenizer

# 1. Очищення тексту
cleaned_text = re.sub(r'[^а-яєґії\s]', ' ', text.lower())
cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

# 2. Ініціалізація токенізатора
tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-base")

# 3. Токенізація
tokenized = tokenizer(cleaned_text, return_tensors="pt", add_special_tokens=False)["input_ids"].squeeze()

# 4. Побудова пар X -> y
seq_length = 64
inputs, targets = [], []

for i in range(len(tokenized) - seq_length):
    inputs.append(tokenized[i:i+seq_length])
    targets.append(tokenized[i+1:i+1+seq_length])

# 5. Перетворення в тензори
inputs = torch.stack(inputs)
targets = torch.stack(targets)

print(f"Кількість токенізованих пар: {len(inputs)}")
print(f"Форма прикладу: {inputs[0].shape} → {targets[0].shape}")

"""##3. Побудова моделі"""

import torch.nn as nn
from transformers import XLMRobertaModel

class TransformerGenerator(nn.Module):
    def __init__(self, hidden_dim, vocab_size, num_layers=2, nhead=8):
        super().__init__()
        self.encoder = XLMRobertaModel.from_pretrained("xlm-roberta-base")
        self.encoder.requires_grad_(False)

        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=nhead)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)

        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.output_fc = nn.Linear(hidden_dim, vocab_size)

        self.hidden_dim = hidden_dim

    def forward(self, input_ids, target_ids):
        # Encoder: отримуємо контекстні ембеддинги з XLM-RoBERTa
        with torch.no_grad():
            encoder_out = self.encoder(input_ids).last_hidden_state.transpose(0, 1)  # [seq, batch, hidden]

        # Decoder: teacher forcing (використовуємо target як вхід)
        tgt_emb = self.embedding(target_ids).transpose(0, 1)  # [seq, batch, hidden]

        tgt_seq_len = target_ids.shape[1]
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(input_ids.device)

        decoder_out = self.transformer_decoder(tgt=tgt_emb, memory=encoder_out, tgt_mask=tgt_mask)
        logits = self.output_fc(decoder_out.transpose(0, 1))  # [batch, seq, vocab]

        return logits

from torch.utils.data import Dataset, DataLoader

class TransformerDataset(Dataset):
    def __init__(self, input_ids, target_ids):
        self.inputs = input_ids
        self.targets = target_ids

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        input_seq = self.inputs[idx]
        target_seq = self.targets[idx]
        return input_seq, target_seq[:-1], target_seq[1:]  # (X, y_in, y_out)

transformer_dataset = TransformerDataset(inputs, targets)
transformer_loader = DataLoader(transformer_dataset, batch_size=16, shuffle=True)

"""##4. Ініціалізація моделі, оптимізатора і функції втрат"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Використовується пристрій:", device)

# Створюємо модель
model = TransformerGenerator(hidden_dim=256, vocab_size=tokenizer.vocab_size).to(device)

# Оптимізатор і функція втрат
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)
criterion = nn.CrossEntropyLoss()

"""##5. Тренування моделі"""

import torch
from tqdm import tqdm
import numpy as np

# Параметри
EPOCHS = 25
PATIENCE = 4
lr = 3e-4
hidden_dim = 768  # має співпадати з XLM-RoBERTa hidden size (зустрічалась помилка раніше)

# Ініціалізація
model = TransformerGenerator(
    hidden_dim=hidden_dim,
    vocab_size=tokenizer.vocab_size,
    num_layers=2,
    nhead=8
).to(device)

optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss()

# Early stopping
best_loss = float("inf")
epochs_no_improve = 0
losses = []

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for input_ids, decoder_in, decoder_out in tqdm(transformer_loader, desc=f"Epoch {epoch+1}"):
        input_ids = input_ids.to(device)
        decoder_in = decoder_in.to(device)
        decoder_out = decoder_out.to(device)

        optimizer.zero_grad()
        logits = model(input_ids, decoder_in)  # [batch, seq, vocab]
        loss = criterion(logits.view(-1, logits.size(-1)), decoder_out.view(-1))
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(transformer_loader)
    losses.append(avg_loss)
    print(f"Epoch {epoch+1} — Loss: {avg_loss:.4f}")

    if avg_loss < best_loss:
        best_loss = avg_loss
        epochs_no_improve = 0
        torch.save(model.state_dict(), "best_transformer.pt")
        print("Модель збережено")
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= PATIENCE:
            print("Early stopping — модель не покращується")
            break

import matplotlib.pyplot as plt

plt.plot(losses)
plt.title("Динаміка втрат")
plt.xlabel("Епоха")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

"""##6. Перевіряємо результат"""

# Завантаження збереженої моделі
model.load_state_dict(torch.load("best_transformer.pt"))
model.eval()

def generate_text_transformer(seed_text, length=50, temperature=1.0):
    model.eval()

    # Токенізуємо seed
    input_ids = tokenizer(seed_text, return_tensors="pt", add_special_tokens=False)["input_ids"].to(device)

    generated_ids = input_ids.clone()

    for _ in range(length):
        # Отримуємо encoder output з XLM-RoBERTa
        with torch.no_grad():
            encoder_out = model.encoder(generated_ids).last_hidden_state.transpose(0, 1)  # [seq_len, batch, hidden]

        # Decoder input (shifted right)
        tgt_input = model.embedding(generated_ids).transpose(0, 1)  # [seq_len, batch, hidden]

        # Casual mask
        seq_len = tgt_input.size(0)
        casual_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(device)

        # Прогін через decoder
        with torch.no_grad():
            decoded = model.transformer_decoder(tgt_input, encoder_out, tgt_mask=casual_mask)
            logits = model.output_fc(decoded[-1])  # беремо тільки останній токен [batch, vocab_size]

        # Sampling (temperature)
        logits = logits / temperature
        probs = torch.nn.functional.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        # Додаємо новий токен
        generated_ids = torch.cat([generated_ids, next_token], dim=1)

        # Обмежуємо довжину до 512 (якщо потрібно)
        if generated_ids.size(1) > 512:
            generated_ids = generated_ids[:, -512:]

    return tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)

print(generate_text_transformer("Сонце", length=100, temperature=1.1))

"""**Висновок.** Загалом модель стабільно навчалась, генерація на основі seed-підказки працює, у принципі модель показує логічні конструкції, доволі іноді зв'язні тексти (залежно від зміни temperature), однак лексичний набір обмежений. Тому потребує покращень

**Пропозиції**:

1.   Покращення існуючої архітектури:
*  Зменшити learning rate та збільшити кількість епох — довше навчання може дозволити моделі краще пристосуватись до особливостей тексту.

*   Інтегрувати beam search або nucleus sampling замість multinomial sampling — це може зменшити часті повтори слів у згенерованому тексті.

*   Збільшити кількість шарів у TransformerDecoder — поточна базова конфігурація з 2 шарів може бути недостатньою для складної мовної генерації.



2.   Збільшення обсягу навчальних даних: використання більшого або більш якісного корпусу українського тексту (наприклад, з художньої літератури) може суттєво розширити словниковий запас моделі.

3. Альтернативні підходи до моделювання:


*   Спробувати іншу pre-trained модель, відмінну від XLM-RoBERTa, наприклад bert-base-multilingual.

*   Навчання з нуля на обраному датасеті замість partial фан-тюнінгу великої моделі — це дозволить краще адаптувати архітектуру під специфіку даних.

*   Модель bidirectional LSTM у попередніх експериментах давала хороші результати — можливо, для цього типу завдання простішої рекурентної архітектури буде достатньо.
"""